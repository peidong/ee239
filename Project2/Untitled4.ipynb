{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "\n",
    "\n",
    "stopwords = text.ENGLISH_STOP_WORDS\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "\n",
    "#load the training data\n",
    "\n",
    "cate = ['comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey']\n",
    "\n",
    "datasets = fetch_20newsgroups(subset = 'train', categories = cate, shuffle = True, random_state = 42, remove = ('headers','footers','quotes'))\n",
    "\n",
    "\n",
    "\n",
    "#load the testing data\n",
    "\n",
    "cate_tt = ['comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey']\n",
    "\n",
    "test_datasets = fetch_20newsgroups(subset = 'test', categories = cate, shuffle = True, random_state = 42, remove = ('headers','footers','quotes'))\n",
    "\n",
    "\n",
    "\n",
    "#sorting the data into 20 different groups, save them in the \"data\" list and get the length of each group\n",
    "\n",
    "index = list()\n",
    "\n",
    "length = list()\n",
    "\n",
    "data = list()\n",
    "\n",
    "for j in range(8):\n",
    "\n",
    "    index_temp = list()\n",
    "\n",
    "    index_temp.append(list(np.where(datasets.target == j))[0])\n",
    "\n",
    "    index.append(index_temp)\n",
    "\n",
    "    data_temp = list()\n",
    "\n",
    "    for i in index[j][0]:\n",
    "\n",
    "        data_temp.append(datasets.data[i])\n",
    "\n",
    "    data.append(data_temp)\n",
    "\n",
    "    length.append(len(data_temp))\n",
    "\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "\n",
    "\n",
    "#make each class have the same number of items\n",
    "\n",
    "for j in range(8):\n",
    "\n",
    "    if j != 3:\n",
    "\n",
    "        data[j][len(data[3]):] = []\n",
    "\n",
    "\n",
    "\n",
    "#combine the data of 8 classes into a new list\n",
    "\n",
    "data_cutoff = list()\n",
    "\n",
    "for j in range(8):\n",
    "\n",
    "    data_cutoff.extend(data[j])\n",
    "\n",
    "\n",
    "\n",
    "#exclude the stop words, punctuations and different stems of a word\n",
    "\n",
    "data_pro = list()\n",
    "\n",
    "for j in range(len(data_cutoff)):\n",
    "\n",
    "    temp = data_cutoff[j]\n",
    "\n",
    "    temp = re.sub(\"[^a-zA-Z]\",\" \",temp)\n",
    "\n",
    "    temp = temp.lower()\n",
    "\n",
    "    words = temp.split()\n",
    "\n",
    "    post_stop = [w for w in words if not w in stopwords]\n",
    "\n",
    "    post_stem = [stemmer.stem(w1) for w1 in post_stop]\n",
    "\n",
    "    temp = \" \".join(post_stem)\n",
    "\n",
    "    data_pro.append(temp)\n",
    "\n",
    "\n",
    "\n",
    "#exclude the stop words, punctuations and different stems of a word for testing set\n",
    "\n",
    "data_test = test_datasets.data[:]\n",
    "\n",
    "data_pro_test = list()\n",
    "\n",
    "for j in range(len(data_test)):\n",
    "\n",
    "    temp_test = data_test[j]\n",
    "\n",
    "    temp_test = re.sub(\"[^a-zA-Z]\",\" \",temp_test)\n",
    "\n",
    "    temp_test = temp_test.lower()\n",
    "\n",
    "    words_test = temp_test.split()\n",
    "\n",
    "    post_stop_test = [w_test for w_test in words_test if not w_test in stopwords]\n",
    "\n",
    "    post_stem_test = [stemmer.stem(w1_test) for w1_test in post_stop_test]\n",
    "\n",
    "    temp_test = \" \".join(post_stem_test)\n",
    "\n",
    "    data_pro_test.append(temp_test)\n",
    "\n",
    "\n",
    "\n",
    "#SVD\n",
    "\n",
    "X = vectorizer.fit_transform(data_pro[:])\n",
    "\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X)\n",
    "\n",
    "X_test = vectorizer.transform(data_pro_test[:])\n",
    "\n",
    "X_test_tfidf = tfidf_transformer.fit_transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "U,s,V = np.linalg.svd(X_train_tfidf.toarray())\n",
    "\n",
    "s1 = s[0:50]\n",
    "\n",
    "U1 = U[:,0:50]\n",
    "\n",
    "V1 = V[0:50,:]\n",
    "\n",
    "D_k_tr = np.dot(X_train_tfidf.toarray(),V1.T)\n",
    "\n",
    "D_k_test = np.dot(X_test_tfidf.toarray(),V1.T)\n",
    "\n",
    "label_tr = np.ones([len(data[3])*8,1])\n",
    "\n",
    "for j in range(0,8):\n",
    "\n",
    "    label_tr[len(data[3])*j:len(data[3])*(j+1)] = j*np.ones([len(data[3]),1])\n",
    "\n",
    "out_tr = np.hstack((D_k_tr,label_tr))\n",
    "\n",
    "\n",
    "\n",
    "label_test = np.zeros([len(D_k_test),1])\n",
    "\n",
    "for j in range(len(D_k_test)):\n",
    "\n",
    "    label_test[j] = test_datasets.target[j]\n",
    "\n",
    "out_test = np.hstack((D_k_test,label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
