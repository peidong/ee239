{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project 2 \n",
    "This explanation are mainly from different sections of the scikit-learn tutorial on text classification available at http://scikit-learn.org."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "1. In this project we work with “20 Newsgroups” dataset. It is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups, each corresponding to a different topic.\n",
    "2. To manually load the data, you need to run this python code.<a href=\"https://www.dropbox.com/s/5oek8qbsge1y64b/fetch_data.py?dl=0\">link to fetch_data.py</a>\n",
    "3. Easiest way to load the data is to use the built-in dataset loader for 20 newsgroups from scikit-learn package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = [ 'comp.graphics', 'comp.sys.mac.hardware']\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "twenty_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def get_Number(fileName):\n",
    "    count_vect = CountVectorizer()\n",
    "    categories = [fileName]\n",
    "    comp_graphic_train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "    comp_graphics_number = count_vect.fit_transform(comp_graphic_train.data)\n",
    "    #print(\"(number of documents, number of features) = (%s, %s) \" % comp_graphics_number.shape)\n",
    "    [r,v] = comp_graphics_number.shape\n",
    "    return r\n",
    "\n",
    "comp_graphics_number = get_Number('comp.graphics')\n",
    "comp_os_mswindows_misc_number = get_Number('comp.os.ms-windows.misc')\n",
    "comp_sys_ibm_pc_hardware_number = get_Number('comp.sys.ibm.pc.hardware')\n",
    "comp_sys_mac_hardware_number = get_Number('comp.sys.mac.hardware')\n",
    "rec_autos_number = get_Number('rec.autos')\n",
    "rec_motorcycles_number = get_Number('rec.motorcycles')\n",
    "rec_sport_baseball_number = get_Number('rec.sport.baseball')\n",
    "rec_sport_hockey_number = get_Number('rec.sport.hockey')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of class is = 2343\n",
      "Number of class is = 2343\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x10dcd0f10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEZCAYAAAB1mUk3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZmY4GZiZGU4GZmYG/H9hce6bo6XfIAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111bba7d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Number_Computer_technology = comp_graphics_number+comp_os_mswindows_misc_number+comp_sys_ibm_pc_hardware_number+comp_sys_mac_hardware_number\n",
    "Number_Recreational_activity = rec_autos_number+rec_motorcycles_number+rec_sport_baseball_number+rec_sport_hockey_number\n",
    "\n",
    "print(\"Number of class is = %s\" % Number_Computer_technology)\n",
    "print(\"Number of class is = %s\" % Number_Computer_technology)\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_groups = 8\n",
    "\n",
    "document_number = (comp_graphics_number, comp_os_mswindows_misc_number, comp_sys_ibm_pc_hardware_number, comp_sys_mac_hardware_number, rec_autos_number, rec_motorcycles_number, rec_sport_baseball_number, rec_sport_hockey_number)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 0.35\n",
    "\n",
    "opacity = 0.4\n",
    "rects = plt.bar(index, document_number, bar_width,alpha=opacity, color='b',label='Topics')\n",
    "\n",
    "plt.xlabel('Topics')\n",
    "plt.ylabel('Number of Documents')\n",
    "plt.title('The Number of Documents per Topic')\n",
    "plt.xticks(index + bar_width, ('Graphics', 'Windows', 'Ibm', 'Mac', 'Autos', 'Motorcycles', 'Baseball', 'Hockey'))\n",
    "plt.ylim(0,800)\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['description', 'DESCR', 'filenames', 'target_names', 'data', 'target']\n"
     ]
    }
   ],
   "source": [
    "# print type(twenty_train)\n",
    "#Use help() command to know more about your object\n",
    "# print help(twenty_train)\n",
    "# print twenty_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: winstead@faraday.ece.cmu.edu (Charles Holden Winstead)\n",
      "Subject: ftp site for Radius software???\n",
      "Organization: Electrical and Computer Engineering, Carnegie Mellon\n",
      "\n",
      "Hey All,\n",
      "\n",
      "Does anyone know if I can ftp to get the newest version of Radiusware\n",
      "and soft pivot from Radius?  I bought a pivot monitor, but it has an\n",
      "old version of this software and won't work on my C650, and Radius said\n",
      "it would be 4-5 weeks until delivery.\n",
      "\n",
      "Thanks!\n",
      "\n",
      "-Chuck\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "comp.sys.mac.hardware\n"
     ]
    }
   ],
   "source": [
    "#print twenty_train.data[0]\n",
    "#print twenty_train.target[0]\n",
    "#print twenty_train.target_names[twenty_train.target[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comp.graphics', 'comp.sys.mac.hardware']\n"
     ]
    }
   ],
   "source": [
    "#print twenty_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files themselves are loaded in memory in the data attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1162\n",
      "1162\n",
      "1162\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#print len(twenty_train.data)\n",
    "#print len(twenty_train.filenames)\n",
    "#print len(twenty_train.target)\n",
    "#print len(twenty_train.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting features from text files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer\n",
    "Convert a collection of text documents to a matrix of token counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "vectorizer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset(['all', 'six', 'less', 'being', 'indeed', 'over', 'move', 'anyway', 'four', 'not', 'own', 'through', 'yourselves', 'fify', 'where', 'mill', 'only', 'find', 'before', 'one', 'whose', 'system', 'how', 'somewhere', 'with', 'thick', 'show', 'had', 'enough', 'should', 'to', 'must', 'whom', 'seeming', 'under', 'ours', 'has', 'might', 'thereafter', 'latterly', 'do', 'them', 'his', 'around', 'than', 'get', 'very', 'de', 'none', 'cannot', 'every', 'whether', 'they', 'front', 'during', 'thus', 'now', 'him', 'nor', 'name', 'several', 'hereafter', 'always', 'who', 'cry', 'whither', 'this', 'someone', 'either', 'each', 'become', 'thereupon', 'sometime', 'side', 'two', 'therein', 'twelve', 'because', 'often', 'ten', 'our', 'eg', 'some', 'back', 'up', 'go', 'namely', 'towards', 'are', 'further', 'beyond', 'ourselves', 'yet', 'out', 'even', 'will', 'what', 'still', 'for', 'bottom', 'mine', 'since', 'please', 'forty', 'per', 'its', 'everything', 'behind', 'un', 'above', 'between', 'it', 'neither', 'seemed', 'ever', 'across', 'she', 'somehow', 'be', 'we', 'full', 'never', 'sixty', 'however', 'here', 'otherwise', 'were', 'whereupon', 'nowhere', 'although', 'found', 'alone', 're', 'along', 'fifteen', 'by', 'both', 'about', 'last', 'would', 'anything', 'via', 'many', 'could', 'thence', 'put', 'against', 'keep', 'etc', 'amount', 'became', 'ltd', 'hence', 'onto', 'or', 'con', 'among', 'already', 'co', 'afterwards', 'formerly', 'within', 'seems', 'into', 'others', 'while', 'whatever', 'except', 'down', 'hers', 'everyone', 'done', 'least', 'another', 'whoever', 'moreover', 'couldnt', 'throughout', 'anyhow', 'yourself', 'three', 'from', 'her', 'few', 'together', 'top', 'there', 'due', 'been', 'next', 'anyone', 'eleven', 'much', 'call', 'therefore', 'interest', 'then', 'thru', 'themselves', 'hundred', 'was', 'sincere', 'empty', 'more', 'himself', 'elsewhere', 'mostly', 'on', 'fire', 'am', 'becoming', 'hereby', 'amongst', 'else', 'part', 'everywhere', 'too', 'herself', 'former', 'those', 'he', 'me', 'myself', 'made', 'twenty', 'these', 'bill', 'cant', 'us', 'until', 'besides', 'nevertheless', 'below', 'anywhere', 'nine', 'can', 'of', 'your', 'toward', 'my', 'something', 'and', 'whereafter', 'whenever', 'give', 'almost', 'wherever', 'is', 'describe', 'beforehand', 'herein', 'an', 'as', 'itself', 'at', 'have', 'in', 'seem', 'whence', 'ie', 'any', 'fill', 'again', 'hasnt', 'inc', 'thereby', 'thin', 'no', 'perhaps', 'latter', 'meanwhile', 'when', 'detail', 'same', 'wherein', 'beside', 'also', 'that', 'other', 'take', 'which', 'becomes', 'you', 'if', 'nobody', 'see', 'though', 'may', 'after', 'upon', 'most', 'hereupon', 'eight', 'but', 'serious', 'nothing', 'such', 'why', 'a', 'off', 'whereby', 'third', 'i', 'whole', 'noone', 'sometimes', 'well', 'amoungst', 'yours', 'their', 'rather', 'without', 'so', 'five', 'the', 'first', 'whereas', 'once'])\n",
      "318\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "stop_words = text.ENGLISH_STOP_WORDS\n",
    "print stop_words\n",
    "print len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x9 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 19 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on CountVectorizer in module sklearn.feature_extraction.text object:\n",
      "\n",
      "class CountVectorizer(sklearn.base.BaseEstimator, VectorizerMixin)\n",
      " |  Convert a collection of text documents to a matrix of token counts\n",
      " |  \n",
      " |  This implementation produces a sparse representation of the counts using\n",
      " |  scipy.sparse.coo_matrix.\n",
      " |  \n",
      " |  If you do not provide an a-priori dictionary and you do not use an analyzer\n",
      " |  that does some kind of feature selection then the number of features will\n",
      " |  be equal to the vocabulary size found by analyzing the data.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  input : string {'filename', 'file', 'content'}\n",
      " |      If 'filename', the sequence passed as an argument to fit is\n",
      " |      expected to be a list of filenames that need reading to fetch\n",
      " |      the raw content to analyze.\n",
      " |  \n",
      " |      If 'file', the sequence items must have a 'read' method (file-like\n",
      " |      object) that is called to fetch the bytes in memory.\n",
      " |  \n",
      " |      Otherwise the input is expected to be the sequence strings or\n",
      " |      bytes items are expected to be analyzed directly.\n",
      " |  \n",
      " |  encoding : string, 'utf-8' by default.\n",
      " |      If bytes or files are given to analyze, this encoding is used to\n",
      " |      decode.\n",
      " |  \n",
      " |  decode_error : {'strict', 'ignore', 'replace'}\n",
      " |      Instruction on what to do if a byte sequence is given to analyze that\n",
      " |      contains characters not of the given `encoding`. By default, it is\n",
      " |      'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
      " |      values are 'ignore' and 'replace'.\n",
      " |  \n",
      " |  strip_accents : {'ascii', 'unicode', None}\n",
      " |      Remove accents during the preprocessing step.\n",
      " |      'ascii' is a fast method that only works on characters that have\n",
      " |      an direct ASCII mapping.\n",
      " |      'unicode' is a slightly slower method that works on any characters.\n",
      " |      None (default) does nothing.\n",
      " |  \n",
      " |  analyzer : string, {'word', 'char', 'char_wb'} or callable\n",
      " |      Whether the feature should be made of word or character n-grams.\n",
      " |      Option 'char_wb' creates character n-grams only from text inside\n",
      " |      word boundaries.\n",
      " |  \n",
      " |      If a callable is passed it is used to extract the sequence of features\n",
      " |      out of the raw, unprocessed input.\n",
      " |  \n",
      " |  preprocessor : callable or None (default)\n",
      " |      Override the preprocessing (string transformation) stage while\n",
      " |      preserving the tokenizing and n-grams generation steps.\n",
      " |  \n",
      " |  tokenizer : callable or None (default)\n",
      " |      Override the string tokenization step while preserving the\n",
      " |      preprocessing and n-grams generation steps.\n",
      " |  \n",
      " |  ngram_range : tuple (min_n, max_n)\n",
      " |      The lower and upper boundary of the range of n-values for different\n",
      " |      n-grams to be extracted. All values of n such that min_n <= n <= max_n\n",
      " |      will be used.\n",
      " |  \n",
      " |  stop_words : string {'english'}, list, or None (default)\n",
      " |      If 'english', a built-in stop word list for English is used.\n",
      " |  \n",
      " |      If a list, that list is assumed to contain stop words, all of which\n",
      " |      will be removed from the resulting tokens.\n",
      " |  \n",
      " |      If None, no stop words will be used. max_df can be set to a value\n",
      " |      in the range [0.7, 1.0) to automatically detect and filter stop\n",
      " |      words based on intra corpus document frequency of terms.\n",
      " |  \n",
      " |  lowercase : boolean, True by default\n",
      " |      Convert all characters to lowercase before tokenizing.\n",
      " |  \n",
      " |  token_pattern : string\n",
      " |      Regular expression denoting what constitutes a \"token\", only used\n",
      " |      if `tokenize == 'word'`. The default regexp select tokens of 2\n",
      " |      or more alphanumeric characters (punctuation is completely ignored\n",
      " |      and always treated as a token separator).\n",
      " |  \n",
      " |  max_df : float in range [0.0, 1.0] or int, default=1.0\n",
      " |      When building the vocabulary ignore terms that have a document\n",
      " |      frequency strictly higher than the given threshold (corpus-specific\n",
      " |      stop words).\n",
      " |      If float, the parameter represents a proportion of documents, integer\n",
      " |      absolute counts.\n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  min_df : float in range [0.0, 1.0] or int, default=1\n",
      " |      When building the vocabulary ignore terms that have a document\n",
      " |      frequency strictly lower than the given threshold. This value is also\n",
      " |      called cut-off in the literature.\n",
      " |      If float, the parameter represents a proportion of documents, integer\n",
      " |      absolute counts.\n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  max_features : int or None, default=None\n",
      " |      If not None, build a vocabulary that only consider the top\n",
      " |      max_features ordered by term frequency across the corpus.\n",
      " |  \n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  vocabulary : Mapping or iterable, optional\n",
      " |      Either a Mapping (e.g., a dict) where keys are terms and values are\n",
      " |      indices in the feature matrix, or an iterable over terms. If not\n",
      " |      given, a vocabulary is determined from the input documents. Indices\n",
      " |      in the mapping should not be repeated and should not have any gap\n",
      " |      between 0 and the largest index.\n",
      " |  \n",
      " |  binary : boolean, default=False\n",
      " |      If True, all non zero counts are set to 1. This is useful for discrete\n",
      " |      probabilistic models that model binary events rather than integer\n",
      " |      counts.\n",
      " |  \n",
      " |  dtype : type, optional\n",
      " |      Type of the matrix returned by fit_transform() or transform().\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  vocabulary_ : dict\n",
      " |      A mapping of terms to feature indices.\n",
      " |  \n",
      " |  stop_words_ : set\n",
      " |      Terms that were ignored because they either:\n",
      " |  \n",
      " |        - occurred in too many documents (`max_df`)\n",
      " |        - occurred in too few documents (`min_df`)\n",
      " |        - were cut off by feature selection (`max_features`).\n",
      " |  \n",
      " |      This is only available if no vocabulary was given.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  HashingVectorizer, TfidfVectorizer\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The ``stop_words_`` attribute can get large and increase the model size\n",
      " |  when pickling. This attribute is provided only for introspection and can\n",
      " |  be safely removed using delattr or set to None before pickling.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CountVectorizer\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      VectorizerMixin\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, input=u'content', encoding=u'utf-8', decode_error=u'strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), analyzer=u'word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<type 'numpy.int64'>)\n",
      " |  \n",
      " |  fit(self, raw_documents, y=None)\n",
      " |      Learn a vocabulary dictionary of all tokens in the raw documents.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which yields either str, unicode or file objects.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  fit_transform(self, raw_documents, y=None)\n",
      " |      Learn the vocabulary dictionary and return term-document matrix.\n",
      " |      \n",
      " |      This is equivalent to fit followed by transform, but more efficiently\n",
      " |      implemented.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which yields either str, unicode or file objects.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : array, [n_samples, n_features]\n",
      " |          Document-term matrix.\n",
      " |  \n",
      " |  get_feature_names(self)\n",
      " |      Array mapping from feature integer indices to feature name\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Return terms per document with nonzero entries in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array, sparse matrix}, shape = [n_samples, n_features]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_inv : list of arrays, len = n_samples\n",
      " |          List of arrays of terms.\n",
      " |  \n",
      " |  transform(self, raw_documents)\n",
      " |      Transform documents to document-term matrix.\n",
      " |      \n",
      " |      Extract token counts out of raw text documents using the vocabulary\n",
      " |      fitted with fit or the one provided to the constructor.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which yields either str, unicode or file objects.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : sparse matrix, [n_samples, n_features]\n",
      " |          Document-term matrix.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep: boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The former have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from VectorizerMixin:\n",
      " |  \n",
      " |  build_analyzer(self)\n",
      " |      Return a callable that handles preprocessing and tokenization\n",
      " |  \n",
      " |  build_preprocessor(self)\n",
      " |      Return a function to preprocess the text before tokenization\n",
      " |  \n",
      " |  build_tokenizer(self)\n",
      " |      Return a function that splits a string into a sequence of tokens\n",
      " |  \n",
      " |  decode(self, doc)\n",
      " |      Decode the input into a string of unicode symbols\n",
      " |      \n",
      " |      The decoding strategy depends on the vectorizer parameters.\n",
      " |  \n",
      " |  get_stop_words(self)\n",
      " |      Build or fetch the effective stop words list\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from VectorizerMixin:\n",
      " |  \n",
      " |  fixed_vocabulary\n",
      " |      DEPRECATED: The `fixed_vocabulary` attribute is deprecated and will be removed in 0.18.  Please use `fixed_vocabulary_` instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 2, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0, 1]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names() == (\n",
    "    ['and', 'document', 'first', 'is', 'one',\n",
    "     'second', 'the', 'third', 'this'])\n",
    "\n",
    "\n",
    "X.toarray()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The converse mapping from feature name to column index is stored in the vocabulary_ attribute of the vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_.get('document')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1162, 19610)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'3ds2scn', u'four', u'compatable', u'circuitry', u'ebth', u'hanging', u'woody', u'spidery', u'shure', u'increase']\n",
      "[(u'3ds2scn', 1349), (u'four', 8380), (u'compatable', 5494), (u'circuitry', 5161), (u'ebth', 7243)]\n",
      "1349\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))\n",
    "\n",
    "\n",
    "print count_vect.vocabulary_.keys()[0:10]\n",
    "print take(5,count_vect.vocabulary_.iteritems())\n",
    "print count_vect.vocabulary_.get('3ds2scn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.08849641,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape\n",
    "X_train_tfidf.toarray()[:30,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a classifier\n",
    "\n",
    "Let's train a classifier to predict the category of a post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'He is an OS developer' => comp.sys.mac.hardware\n",
      "'OpenGL on the GPU is fast' => comp.graphics\n"
     ]
    }
   ],
   "source": [
    "docs_new = ['He is an OS developer', 'OpenGL on the GPU is fast']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, twenty_train.target_names[category]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a pipeline\n",
    "In order to make the vectorizer => transformer => classifier easier to work with, scikit-learn provides a Pipeline class that behaves like a compound classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_clf = text_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9315245478036176"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "twenty_test = fetch_20newsgroups(subset='test',\n",
    "    categories=categories, shuffle=True, random_state=42)\n",
    "docs_test = twenty_test.data\n",
    "predicted = text_clf.predict(docs_test)\n",
    "np.mean(predicted == twenty_test.target) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      "        comp.graphics       0.97      0.89      0.93       389\n",
      "comp.sys.mac.hardware       0.90      0.97      0.93       385\n",
      "\n",
      "          avg / total       0.93      0.93      0.93       774\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[346,  43],\n",
       "       [ 10, 375]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(twenty_test.target, predicted,\n",
    "    target_names=twenty_test.target_names))\n",
    "\n",
    "metrics.confusion_matrix(twenty_test.target, predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
